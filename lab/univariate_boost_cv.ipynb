{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script com foco no processo de forecast com previsão multi-step. <br>Método: direct prediction (ver https://machinelearningmastery.com/multi-step-time-series-forecasting/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=fG8H-0rb0mY\n",
    "#https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import json\n",
    "\n",
    "import lightgbm as lgb\n",
    "#lgb.__version__\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight') # estilo dos gráficos\n",
    "rcParams['figure.figsize'] = 15, 5 # tamanho das figuras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Função para ler e transformar os dados já presentes no diretório especificado\n",
    "    \"\"\"\n",
    "    path = \"../data/daily_load.csv\"\n",
    "    df_load = pd.read_csv(path, parse_dates = [\"date\"])\n",
    "    df_load2 = df_load[df_load[\"id_reg\"] == \"S\"]           # região sul\n",
    "    df_load3 = df_load2[df_load2[\"date\"] <= '2022-05-31']  # data de corte\n",
    "    df_load4 = df_load3[[\"date\", \"load_mwmed\"]].set_index(\"date\")\n",
    "    return df_load4\n",
    "\n",
    "def train_test_split(data, n_test):\n",
    "    \"\"\"\n",
    "    Função para partir or dados em treino e teste\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        train, test = data.iloc[:-n_test, :], data.iloc[-n_test:, :]\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        train, test = data[:-n_test, :], data[-n_test:, :]\n",
    "    return train, test\n",
    "\n",
    "# https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "\n",
    "# transform a time series dataset into a supervised learning dataset\n",
    "def series_to_supervised(data, n_in = 1, n_out = 1, dropnan = True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis = 1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace = True)\n",
    "    return agg\n",
    "\n",
    "def lightgbm_forecast(train, testX):\n",
    "\t# transform list into array\n",
    "\ttrain = np.asarray(train)\n",
    "\t# split into input and output columns\n",
    "\ttrainX, trainy = train[:, :-1], train[:, -1]\n",
    "\t# fit model\n",
    "\tmodel = lgb.LGBMRegressor(objective='regression', n_estimators=1000)\n",
    "\tmodel.fit(trainX, trainy)\n",
    "\t# make a one-step prediction\n",
    "\tyhat = model.predict([testX])\n",
    "\treturn yhat[0]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test):\n",
    "    predictions = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # split test row into input and output columns\n",
    "        testX, testy = test[i, :-1], test[i, -1]\n",
    "        # fit model on history and make a prediction\n",
    "        yhat = lightgbm_forecast(history, testX)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "        # summarize progress\n",
    "        print('>expected = %.1f, predicted = %.1f' % (testy, yhat))\n",
    "    # estimate prediction error\n",
    "    mae = mean_absolute_error(test[:, -1], predictions)\n",
    "    mape = mean_absolute_percentage_error(test[:, -1], predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(test[:, -1], predictions))    \n",
    "    return mae, mape, rmse, test[:, -1], predictions\n",
    "\n",
    "def get_measures(forecast, test):\n",
    "    \"\"\"\n",
    "    Função para obter medidas de acurária a partir dos dados de projeção e teste\n",
    "    \"\"\"\n",
    "    #forecast.reset_index(drop = True, inplace = True)\n",
    "    #test.reset_index(drop = True, inplace = True)\n",
    "    #errors = [(test.iloc[i] - forecast.iloc[i])**2 for i in range(len(test))]\n",
    "    if isinstance(forecast, pd.Series) and isinstance(test, pd.Series):\n",
    "        errors = [(test.iloc[i] - forecast.iloc[i])**2 for i in range(len(test))]\n",
    "    # else:\n",
    "    #     errors = [(test.iloc[i][0] - forecast.iloc[i])**2 for i in range(len(test))]\n",
    "    mae = mean_absolute_error(test, forecast)\n",
    "    mse = mean_squared_error(test, forecast)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(test, forecast)\n",
    "    # smape\n",
    "    a = np.reshape(test.values, (-1,))\n",
    "    b = np.reshape(forecast.values, (-1,))\n",
    "    smape = np.mean(100*2.0 * np.abs(a - b) / (np.abs(a) + np.abs(b))).item()\n",
    "    # dicionário com as medidas de erro\n",
    "    measures = { \"erro\": sum(errors),\n",
    "                 \"mae\": mae,\n",
    "                 \"mse\": mse,\n",
    "                 \"rmse\": rmse,\n",
    "                 \"mape\": mape,\n",
    "                 \"smape\": smape\n",
    "                }\n",
    "    # arredondamento\n",
    "    # for key, item in measures.items():\n",
    "    #     measures[key] = round(measures[key], 2)\n",
    "    return measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANUALMENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df.interpolate(method = \"linear\", inplace = True)\n",
    "values = df.values.tolist()\n",
    "lag = 10 # número de variáveis explicativas\n",
    "n_test = outs = 5 # tamanho da partição de teste (= número de variáveis explicadas)\n",
    "data = series_to_supervised(values, n_in = lag, n_out = outs, dropnan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_test = 31\n",
    "train, test = train_test_split(data, n_test)\n",
    "train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-10)</th>\n",
       "      <th>var1(t-9)</th>\n",
       "      <th>var1(t-8)</th>\n",
       "      <th>var1(t-7)</th>\n",
       "      <th>var1(t-6)</th>\n",
       "      <th>var1(t-5)</th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>var1(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var1(t+1)</th>\n",
       "      <th>var1(t+2)</th>\n",
       "      <th>var1(t+3)</th>\n",
       "      <th>var1(t+4)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4800.650000</td>\n",
       "      <td>4899.800000</td>\n",
       "      <td>6261.554167</td>\n",
       "      <td>6733.741667</td>\n",
       "      <td>6961.170833</td>\n",
       "      <td>7110.362500</td>\n",
       "      <td>7105.354167</td>\n",
       "      <td>6307.487500</td>\n",
       "      <td>5523.620833</td>\n",
       "      <td>7111.320833</td>\n",
       "      <td>7435.058333</td>\n",
       "      <td>7425.491667</td>\n",
       "      <td>7505.575000</td>\n",
       "      <td>7532.275000</td>\n",
       "      <td>6435.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4899.800000</td>\n",
       "      <td>6261.554167</td>\n",
       "      <td>6733.741667</td>\n",
       "      <td>6961.170833</td>\n",
       "      <td>7110.362500</td>\n",
       "      <td>7105.354167</td>\n",
       "      <td>6307.487500</td>\n",
       "      <td>5523.620833</td>\n",
       "      <td>7111.320833</td>\n",
       "      <td>7435.058333</td>\n",
       "      <td>7425.491667</td>\n",
       "      <td>7505.575000</td>\n",
       "      <td>7532.275000</td>\n",
       "      <td>6435.912500</td>\n",
       "      <td>5621.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6261.554167</td>\n",
       "      <td>6733.741667</td>\n",
       "      <td>6961.170833</td>\n",
       "      <td>7110.362500</td>\n",
       "      <td>7105.354167</td>\n",
       "      <td>6307.487500</td>\n",
       "      <td>5523.620833</td>\n",
       "      <td>7111.320833</td>\n",
       "      <td>7435.058333</td>\n",
       "      <td>7425.491667</td>\n",
       "      <td>7505.575000</td>\n",
       "      <td>7532.275000</td>\n",
       "      <td>6435.912500</td>\n",
       "      <td>5621.175000</td>\n",
       "      <td>7234.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6733.741667</td>\n",
       "      <td>6961.170833</td>\n",
       "      <td>7110.362500</td>\n",
       "      <td>7105.354167</td>\n",
       "      <td>6307.487500</td>\n",
       "      <td>5523.620833</td>\n",
       "      <td>7111.320833</td>\n",
       "      <td>7435.058333</td>\n",
       "      <td>7425.491667</td>\n",
       "      <td>7505.575000</td>\n",
       "      <td>7532.275000</td>\n",
       "      <td>6435.912500</td>\n",
       "      <td>5621.175000</td>\n",
       "      <td>7234.966667</td>\n",
       "      <td>7517.372917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6961.170833</td>\n",
       "      <td>7110.362500</td>\n",
       "      <td>7105.354167</td>\n",
       "      <td>6307.487500</td>\n",
       "      <td>5523.620833</td>\n",
       "      <td>7111.320833</td>\n",
       "      <td>7435.058333</td>\n",
       "      <td>7425.491667</td>\n",
       "      <td>7505.575000</td>\n",
       "      <td>7532.275000</td>\n",
       "      <td>6435.912500</td>\n",
       "      <td>5621.175000</td>\n",
       "      <td>7234.966667</td>\n",
       "      <td>7517.372917</td>\n",
       "      <td>7391.795833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178</th>\n",
       "      <td>11361.001881</td>\n",
       "      <td>11305.707062</td>\n",
       "      <td>10060.549787</td>\n",
       "      <td>8994.856748</td>\n",
       "      <td>11424.087542</td>\n",
       "      <td>11964.909375</td>\n",
       "      <td>12269.051375</td>\n",
       "      <td>12021.415458</td>\n",
       "      <td>11802.526458</td>\n",
       "      <td>10256.970375</td>\n",
       "      <td>8938.579125</td>\n",
       "      <td>11713.104333</td>\n",
       "      <td>12054.195042</td>\n",
       "      <td>12186.721375</td>\n",
       "      <td>12482.523708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8179</th>\n",
       "      <td>11305.707062</td>\n",
       "      <td>10060.549787</td>\n",
       "      <td>8994.856748</td>\n",
       "      <td>11424.087542</td>\n",
       "      <td>11964.909375</td>\n",
       "      <td>12269.051375</td>\n",
       "      <td>12021.415458</td>\n",
       "      <td>11802.526458</td>\n",
       "      <td>10256.970375</td>\n",
       "      <td>8938.579125</td>\n",
       "      <td>11713.104333</td>\n",
       "      <td>12054.195042</td>\n",
       "      <td>12186.721375</td>\n",
       "      <td>12482.523708</td>\n",
       "      <td>12520.803833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8180</th>\n",
       "      <td>10060.549787</td>\n",
       "      <td>8994.856748</td>\n",
       "      <td>11424.087542</td>\n",
       "      <td>11964.909375</td>\n",
       "      <td>12269.051375</td>\n",
       "      <td>12021.415458</td>\n",
       "      <td>11802.526458</td>\n",
       "      <td>10256.970375</td>\n",
       "      <td>8938.579125</td>\n",
       "      <td>11713.104333</td>\n",
       "      <td>12054.195042</td>\n",
       "      <td>12186.721375</td>\n",
       "      <td>12482.523708</td>\n",
       "      <td>12520.803833</td>\n",
       "      <td>10525.490875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8181</th>\n",
       "      <td>8994.856748</td>\n",
       "      <td>11424.087542</td>\n",
       "      <td>11964.909375</td>\n",
       "      <td>12269.051375</td>\n",
       "      <td>12021.415458</td>\n",
       "      <td>11802.526458</td>\n",
       "      <td>10256.970375</td>\n",
       "      <td>8938.579125</td>\n",
       "      <td>11713.104333</td>\n",
       "      <td>12054.195042</td>\n",
       "      <td>12186.721375</td>\n",
       "      <td>12482.523708</td>\n",
       "      <td>12520.803833</td>\n",
       "      <td>10525.490875</td>\n",
       "      <td>9074.211250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8182</th>\n",
       "      <td>11424.087542</td>\n",
       "      <td>11964.909375</td>\n",
       "      <td>12269.051375</td>\n",
       "      <td>12021.415458</td>\n",
       "      <td>11802.526458</td>\n",
       "      <td>10256.970375</td>\n",
       "      <td>8938.579125</td>\n",
       "      <td>11713.104333</td>\n",
       "      <td>12054.195042</td>\n",
       "      <td>12186.721375</td>\n",
       "      <td>12482.523708</td>\n",
       "      <td>12520.803833</td>\n",
       "      <td>10525.490875</td>\n",
       "      <td>9074.211250</td>\n",
       "      <td>11648.709583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8173 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        var1(t-10)     var1(t-9)     var1(t-8)     var1(t-7)     var1(t-6)  \\\n",
       "10     4800.650000   4899.800000   6261.554167   6733.741667   6961.170833   \n",
       "11     4899.800000   6261.554167   6733.741667   6961.170833   7110.362500   \n",
       "12     6261.554167   6733.741667   6961.170833   7110.362500   7105.354167   \n",
       "13     6733.741667   6961.170833   7110.362500   7105.354167   6307.487500   \n",
       "14     6961.170833   7110.362500   7105.354167   6307.487500   5523.620833   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "8178  11361.001881  11305.707062  10060.549787   8994.856748  11424.087542   \n",
       "8179  11305.707062  10060.549787   8994.856748  11424.087542  11964.909375   \n",
       "8180  10060.549787   8994.856748  11424.087542  11964.909375  12269.051375   \n",
       "8181   8994.856748  11424.087542  11964.909375  12269.051375  12021.415458   \n",
       "8182  11424.087542  11964.909375  12269.051375  12021.415458  11802.526458   \n",
       "\n",
       "         var1(t-5)     var1(t-4)     var1(t-3)     var1(t-2)     var1(t-1)  \\\n",
       "10     7110.362500   7105.354167   6307.487500   5523.620833   7111.320833   \n",
       "11     7105.354167   6307.487500   5523.620833   7111.320833   7435.058333   \n",
       "12     6307.487500   5523.620833   7111.320833   7435.058333   7425.491667   \n",
       "13     5523.620833   7111.320833   7435.058333   7425.491667   7505.575000   \n",
       "14     7111.320833   7435.058333   7425.491667   7505.575000   7532.275000   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "8178  11964.909375  12269.051375  12021.415458  11802.526458  10256.970375   \n",
       "8179  12269.051375  12021.415458  11802.526458  10256.970375   8938.579125   \n",
       "8180  12021.415458  11802.526458  10256.970375   8938.579125  11713.104333   \n",
       "8181  11802.526458  10256.970375   8938.579125  11713.104333  12054.195042   \n",
       "8182  10256.970375   8938.579125  11713.104333  12054.195042  12186.721375   \n",
       "\n",
       "           var1(t)     var1(t+1)     var1(t+2)     var1(t+3)     var1(t+4)  \n",
       "10     7435.058333   7425.491667   7505.575000   7532.275000   6435.912500  \n",
       "11     7425.491667   7505.575000   7532.275000   6435.912500   5621.175000  \n",
       "12     7505.575000   7532.275000   6435.912500   5621.175000   7234.966667  \n",
       "13     7532.275000   6435.912500   5621.175000   7234.966667   7517.372917  \n",
       "14     6435.912500   5621.175000   7234.966667   7517.372917   7391.795833  \n",
       "...            ...           ...           ...           ...           ...  \n",
       "8178   8938.579125  11713.104333  12054.195042  12186.721375  12482.523708  \n",
       "8179  11713.104333  12054.195042  12186.721375  12482.523708  12520.803833  \n",
       "8180  12054.195042  12186.721375  12482.523708  12520.803833  10525.490875  \n",
       "8181  12186.721375  12482.523708  12520.803833  10525.490875   9074.211250  \n",
       "8182  12482.523708  12520.803833  10525.490875   9074.211250  11648.709583  \n",
       "\n",
       "[8173 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 5: 2000-01-01 to 2022-04-11\n",
      "  test set 5: 2022-04-12 to 2022-04-21\n",
      "training set 4: 2000-01-01 to 2022-04-21\n",
      "  test set 4: 2022-04-22 to 2022-05-01\n",
      "training set 3: 2000-01-01 to 2022-05-01\n",
      "  test set 3: 2022-05-02 to 2022-05-11\n",
      "training set 2: 2000-01-01 to 2022-05-11\n",
      "  test set 2: 2022-05-12 to 2022-05-21\n",
      "training set 1: 2000-01-01 to 2022-05-21\n",
      "  test set 1: 2022-05-22 to 2022-05-31\n"
     ]
    }
   ],
   "source": [
    "# OK\n",
    "folds = 5\n",
    "h = 10\n",
    "rows = df.shape[0]\n",
    "for fold in range(folds,0,-1):\n",
    "    #print(fold)\n",
    "    #print(df.index[rows-(fold*h)-1])\n",
    "    slide = rows-(fold*h)#-1\n",
    "    train = df.iloc[:slide]\n",
    "    #print(range_.tail())\n",
    "    tr_init = df.index[0].date()\n",
    "    tr_end = train.index[-1].date()\n",
    "    te_init = df.index[train.shape[0]].date()\n",
    "    te_end = df.index[train.shape[0]+(h-1)].date()\n",
    "    print(f\"training set {fold}: {tr_init} to {tr_end}\\n  test set {fold}: {te_init} to {te_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting for cv 5...\n",
      "    Predicting var1(t)\n",
      "  > expected: 11964.909375, predicted: 11975.427756245648\n",
      "    Predicting var1(t+1)\n",
      "  > expected: 12269.051375, predicted: 11662.180552659058\n",
      "    Predicting var1(t+2)\n",
      "  > expected: 12021.41545833, predicted: 11648.877597875558\n",
      "    Predicting var1(t+3)\n",
      "  > expected: 11802.52645833, predicted: 11479.928545194285\n",
      "    Predicting var1(t+4)\n",
      "  > expected: 10256.970375, predicted: 9807.548458155632\n",
      "    Predicting var1(t+5)\n",
      "  > expected: 8938.579125, predicted: 8618.267974395803\n",
      "    Predicting var1(t+6)\n",
      "  > expected: 11713.10433333, predicted: 10855.438173683217\n",
      "    Predicting var1(t+7)\n",
      "  > expected: 12054.19504167, predicted: 11393.44424901352\n",
      "    Predicting var1(t+8)\n",
      "  > expected: 12186.721375, predicted: 11548.779196824977\n",
      "    Predicting var1(t+9)\n",
      "  > expected: 12482.52370833, predicted: 10964.932685504376\n",
      "    Predicting var1(t+10)\n",
      "  > expected: 12520.80383333, predicted: 11741.521500258614\n",
      "    Predicting var1(t+11)\n",
      "  > expected: 10525.490875, predicted: 9743.052473270089\n",
      "    Predicting var1(t+12)\n",
      "  > expected: 9074.21125, predicted: 8470.999101812638\n",
      "    Predicting var1(t+13)\n",
      "  > expected: 11648.70958333, predicted: 11021.455950867195\n",
      "    Predicting var1(t+14)\n",
      "  > expected: 12162.75679167, predicted: 11772.039310028269\n",
      "predicting for cv 4...\n",
      "    Predicting var1(t)\n",
      "  > expected: 11964.909375, predicted: 11975.427756245648\n",
      "    Predicting var1(t+1)\n",
      "  > expected: 12269.051375, predicted: 11662.180552659058\n",
      "    Predicting var1(t+2)\n",
      "  > expected: 12021.41545833, predicted: 11648.877597875558\n",
      "    Predicting var1(t+3)\n",
      "  > expected: 11802.52645833, predicted: 11479.928545194285\n",
      "    Predicting var1(t+4)\n",
      "  > expected: 10256.970375, predicted: 9807.548458155632\n",
      "    Predicting var1(t+5)\n",
      "  > expected: 8938.579125, predicted: 8618.267974395803\n",
      "    Predicting var1(t+6)\n",
      "  > expected: 11713.10433333, predicted: 10855.438173683217\n",
      "    Predicting var1(t+7)\n",
      "  > expected: 12054.19504167, predicted: 11393.44424901352\n",
      "    Predicting var1(t+8)\n",
      "  > expected: 12186.721375, predicted: 11548.779196824977\n",
      "    Predicting var1(t+9)\n",
      "  > expected: 12482.52370833, predicted: 10964.932685504376\n",
      "    Predicting var1(t+10)\n",
      "  > expected: 12520.80383333, predicted: 11741.521500258614\n",
      "    Predicting var1(t+11)\n",
      "  > expected: 10525.490875, predicted: 9743.052473270089\n",
      "    Predicting var1(t+12)\n",
      "  > expected: 9074.21125, predicted: 8470.999101812638\n",
      "    Predicting var1(t+13)\n",
      "  > expected: 11648.70958333, predicted: 11021.455950867195\n",
      "    Predicting var1(t+14)\n",
      "  > expected: 12162.75679167, predicted: 11772.039310028269\n",
      "predicting for cv 3...\n",
      "    Predicting var1(t)\n",
      "  > expected: 11964.909375, predicted: 11975.427756245648\n",
      "    Predicting var1(t+1)\n",
      "  > expected: 12269.051375, predicted: 11662.180552659058\n",
      "    Predicting var1(t+2)\n",
      "  > expected: 12021.41545833, predicted: 11648.877597875558\n",
      "    Predicting var1(t+3)\n",
      "  > expected: 11802.52645833, predicted: 11479.928545194285\n",
      "    Predicting var1(t+4)\n",
      "  > expected: 10256.970375, predicted: 9807.548458155632\n",
      "    Predicting var1(t+5)\n",
      "  > expected: 8938.579125, predicted: 8618.267974395803\n",
      "    Predicting var1(t+6)\n",
      "  > expected: 11713.10433333, predicted: 10855.438173683217\n",
      "    Predicting var1(t+7)\n",
      "  > expected: 12054.19504167, predicted: 11393.44424901352\n",
      "    Predicting var1(t+8)\n",
      "  > expected: 12186.721375, predicted: 11548.779196824977\n",
      "    Predicting var1(t+9)\n",
      "  > expected: 12482.52370833, predicted: 10964.932685504376\n",
      "    Predicting var1(t+10)\n",
      "  > expected: 12520.80383333, predicted: 11741.521500258614\n",
      "    Predicting var1(t+11)\n",
      "  > expected: 10525.490875, predicted: 9743.052473270089\n",
      "    Predicting var1(t+12)\n",
      "  > expected: 9074.21125, predicted: 8470.999101812638\n",
      "    Predicting var1(t+13)\n",
      "  > expected: 11648.70958333, predicted: 11021.455950867195\n",
      "    Predicting var1(t+14)\n",
      "  > expected: 12162.75679167, predicted: 11772.039310028269\n",
      "predicting for cv 2...\n",
      "    Predicting var1(t)\n",
      "  > expected: 11964.909375, predicted: 11975.427756245648\n",
      "    Predicting var1(t+1)\n",
      "  > expected: 12269.051375, predicted: 11662.180552659058\n",
      "    Predicting var1(t+2)\n",
      "  > expected: 12021.41545833, predicted: 11648.877597875558\n",
      "    Predicting var1(t+3)\n",
      "  > expected: 11802.52645833, predicted: 11479.928545194285\n",
      "    Predicting var1(t+4)\n",
      "  > expected: 10256.970375, predicted: 9807.548458155632\n",
      "    Predicting var1(t+5)\n",
      "  > expected: 8938.579125, predicted: 8618.267974395803\n",
      "    Predicting var1(t+6)\n",
      "  > expected: 11713.10433333, predicted: 10855.438173683217\n",
      "    Predicting var1(t+7)\n",
      "  > expected: 12054.19504167, predicted: 11393.44424901352\n",
      "    Predicting var1(t+8)\n",
      "  > expected: 12186.721375, predicted: 11548.779196824977\n",
      "    Predicting var1(t+9)\n",
      "  > expected: 12482.52370833, predicted: 10964.932685504376\n",
      "    Predicting var1(t+10)\n",
      "  > expected: 12520.80383333, predicted: 11741.521500258614\n",
      "    Predicting var1(t+11)\n",
      "  > expected: 10525.490875, predicted: 9743.052473270089\n",
      "    Predicting var1(t+12)\n",
      "  > expected: 9074.21125, predicted: 8470.999101812638\n",
      "    Predicting var1(t+13)\n",
      "  > expected: 11648.70958333, predicted: 11021.455950867195\n",
      "    Predicting var1(t+14)\n",
      "  > expected: 12162.75679167, predicted: 11772.039310028269\n",
      "predicting for cv 1...\n",
      "    Predicting var1(t)\n",
      "  > expected: 11964.909375, predicted: 11975.427756245648\n",
      "    Predicting var1(t+1)\n",
      "  > expected: 12269.051375, predicted: 11662.180552659058\n",
      "    Predicting var1(t+2)\n",
      "  > expected: 12021.41545833, predicted: 11648.877597875558\n",
      "    Predicting var1(t+3)\n",
      "  > expected: 11802.52645833, predicted: 11479.928545194285\n",
      "    Predicting var1(t+4)\n",
      "  > expected: 10256.970375, predicted: 9807.548458155632\n",
      "    Predicting var1(t+5)\n",
      "  > expected: 8938.579125, predicted: 8618.267974395803\n",
      "    Predicting var1(t+6)\n",
      "  > expected: 11713.10433333, predicted: 10855.438173683217\n",
      "    Predicting var1(t+7)\n",
      "  > expected: 12054.19504167, predicted: 11393.44424901352\n",
      "    Predicting var1(t+8)\n",
      "  > expected: 12186.721375, predicted: 11548.779196824977\n",
      "    Predicting var1(t+9)\n",
      "  > expected: 12482.52370833, predicted: 10964.932685504376\n",
      "    Predicting var1(t+10)\n",
      "  > expected: 12520.80383333, predicted: 11741.521500258614\n",
      "    Predicting var1(t+11)\n",
      "  > expected: 10525.490875, predicted: 9743.052473270089\n",
      "    Predicting var1(t+12)\n",
      "  > expected: 9074.21125, predicted: 8470.999101812638\n",
      "    Predicting var1(t+13)\n",
      "  > expected: 11648.70958333, predicted: 11021.455950867195\n",
      "    Predicting var1(t+14)\n",
      "  > expected: 12162.75679167, predicted: 11772.039310028269\n"
     ]
    }
   ],
   "source": [
    "# OK\n",
    "\n",
    "df = load_data()\n",
    "df.interpolate(method = \"linear\", inplace = True)\n",
    "\n",
    "folds = 5 #partições\n",
    "horz = 15 #horizonte de predição\n",
    "rows = df.shape[0]\n",
    "walkin = dict()\n",
    "for fold in range(folds,0,-1):\n",
    "    slide = rows-(fold*horz)#-1\n",
    "    train = df.iloc[:slide]\n",
    "\n",
    "    values = df.values.tolist()\n",
    "    lag = 60 # número de variáveis explicativas\n",
    "    n_test = outs = horz # tamanho da partição de teste (= número de variáveis explicadas)\n",
    "    data = series_to_supervised(values, n_in = lag, n_out = outs, dropnan=False)\n",
    "\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    train.dropna(inplace = True)\n",
    "\n",
    "    response_vars = data.columns[-(outs):]\n",
    "    predictions = list()\n",
    "    print(f\"predicting for cv {fold}...\")\n",
    "    for h, response in enumerate(response_vars):\n",
    "        cols = [x for x in data.columns[:lag]]\n",
    "        varname = response\n",
    "        cols.append(varname)\n",
    "        data_ = train[cols]\n",
    "        nrows = data_.shape[0]\n",
    "        data_ = data_.iloc[:nrows-h, :] \n",
    "        data_X, data_y = data_.iloc[:, :-1], data_.iloc[:, -1]\n",
    "        model = lgb.LGBMRegressor(objective='regression', n_estimators=1000)\n",
    "        model.fit(data_X, data_y)\n",
    "        #teste = train.loc[:, :\"var1(t-1)\"].iloc[-1,:] # t + 3 (observado = 12.054,20)\n",
    "        testX, testy = test.reset_index(drop=True).loc[0, :\"var1(t-1)\"], test.reset_index(drop=True).loc[0, varname]\n",
    "        pred = model.predict([testX])[0]\n",
    "        print(f\"    Predicting {varname}\\n  > expected: {testy}, predicted: {pred}\")\n",
    "        predictions.append(pred)\n",
    "    walkin[f\"cv_{fold}\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
